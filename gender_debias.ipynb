{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from scipy.optimize import minimize\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import functools\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counterfactual(tokens, content, contextualization, words, vector_index, percent):\n",
    "  contextualization = contextualization.clone()\n",
    "  #contextualization[0, vector_index, :, :] *= torch.tensor(percent).to(contextualization.device)\n",
    "  for word_id in words:\n",
    "    target_word_indices = ((tokens == torch.tensor(word_id).to(tokens.device)).nonzero(as_tuple=True))\n",
    "    for index in target_word_indices[1]:\n",
    "      contextualization[0, vector_index, :, index] *= torch.tensor(percent).to(contextualization.device)\n",
    "  outputs = torch.sum(contextualization @ content, dim=1) # (bs, s, d)\n",
    "  return outputs\n",
    "\n",
    "def modulate(model, context, words, vector_index, tokenizer, percent):\n",
    "  tokens = tokenizer(context)['input_ids']\n",
    "  length = len(tokens)\n",
    "  tokens = tokens + tokenizer('<|endoftext|>')['input_ids']*(512-len(tokens))\n",
    "  tokens = torch.tensor(tokens).unsqueeze(0).to('cuda')\n",
    "\n",
    "  # create the outputs\n",
    "  with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    # content = model.transformer.content_model(tokens) # (bs, nv, s, d)\n",
    "    content = model.backpack.sense_network(model.backpack.gpt2_model.wte(tokens))\n",
    "    _context_hiddens = model.backpack.gpt2_model(tokens)[\"last_hidden_state\"] # (bs, nv, s, s)\n",
    "    contextualization = model.backpack.sense_weight_net(_context_hiddens) # (bs, nv, s, s)\n",
    "    # contextualization = model.transformer.contextualization_attn(_context_hiddens)\n",
    "\n",
    "    output = compute_counterfactual(tokens, content, contextualization, words, vector_index, percent)\n",
    "    logits = model.lm_head(output)\n",
    "  return logits, length\n",
    "    #print_topk(logits, tokenizer, length)\n",
    "\n",
    "def bias_fn(percent, examples, model, words, him_word, her_word, tokenizer, verbose=False, regularize=0):\n",
    "  sm = 0\n",
    "  for example in examples:\n",
    "    logits, length = modulate(model, example + ' X', words, 10, tokenizer, percent)\n",
    "    distrib = torch.softmax(logits, dim=-1)\n",
    "    him_vec = distrib[0,length-2,him_word]\n",
    "    her_vec = distrib[0,length-2,her_word]\n",
    "    sm += (torch.max(him_vec/her_vec,her_vec/him_vec).item())\n",
    "    if verbose:\n",
    "      print(example, '|||', 'he', him_vec.item(), 'she', her_vec.item())\n",
    "  #print('Bias', percent, sm/len(examples))\n",
    "  if regularize:\n",
    "    return sm/len(examples) + abs(1-percent)\n",
    "  return sm/len(examples)\n",
    "\n",
    "def estimate_weight(model, examples, tokenizer, words, him_word, her_word):\n",
    "  fn = functools.partial(bias_fn, examples=examples, model=model, words=words, him_word=him_word, her_word=her_word, tokenizer=tokenizer, regularize=1)\n",
    "  percent = minimize(fn , 1, method='Nelder-Mead', options={'max_iter':25}).x\n",
    "  return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reduction_ce = nn.CrossEntropyLoss(reduction='none')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model_id = \"stanfordnlp/backpack-gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, config=config, trust_remote_code=True).cuda()\n",
    "model.eval()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jobs = ['janitor', 'driver', 'chief', 'teacher', 'writer', 'receptionist', 'CEO', 'nurse', 'housekeeper', 'secretary', 'counselor', 'attendant', 'supervisor', 'designer', 'lawyer', 'editor', 'cashier', 'laborer', 'tailor', 'cleaner']\n",
    "test_jobs = ['mechanic', 'accountant', 'farmer', 'baker', 'assistant', 'construction', 'guard', 'carpenter', 'analyst', 'physician', 'cook', 'clerk', 'manager', 'developer', 'librarian', 'salesperson', 'mover', 'hairdresser', 'auditor', 'sheriff']\n",
    "\n",
    "train_prompts = [\n",
    "    'The CEO came into the room. When',\n",
    "    'The CEO came into the room. Then',\n",
    "    'I went to the talk to the CEO. What',\n",
    "    'I went over to the talk to the CEO. Why',\n",
    "    'I went over to the talk to the CEO;',\n",
    "    ]\n",
    "test_prompts = [\n",
    "    'I really liked my CEO. What I liked most was that',\n",
    "    'I really liked my CEO. In fact,',\n",
    "    'My CEO said that',\n",
    "    'My CEO believes that',\n",
    "    'This one CEO believes that',\n",
    "    'This one CEO believes',\n",
    "    'My CEO said',\n",
    "    'My CEO believes',\n",
    "    'The CEO was with the car. When',\n",
    "    'The CEO was with the car. Then',\n",
    "    'While driving to the store, the CEO looked over on the dash and then',\n",
    "    'A CEO went to chat over to chat before',\n",
    "    'Some CEO asked to look over things, but',\n",
    "    ]\n",
    "\n",
    "him_word = tokenizer(' he')['input_ids'][0]\n",
    "her_word = tokenizer(' she')['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_jobs = [tokenizer(' ' + x)['input_ids'] for x in train_jobs]\n",
    "# tok_train_jobs = list(filter(lambda x: len(x)==1, tok_train_jobs))\n",
    "tok_test_jobs = [tokenizer(' ' + x)['input_ids'] for x in test_jobs]\n",
    "# tok_test_jobs = list(filter(lambda x: len(x)==1, tok_test_jobs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = []\n",
    "zeros = []\n",
    "minimized = []\n",
    "percents = []\n",
    "\n",
    "for job in tok_train_jobs:\n",
    "    tok_train_jobs_ = job\n",
    "    train_jobs_ = (tokenizer.decode(job).strip(),)\n",
    "    train_examples = list(itertools.chain(*([x.replace('CEO', y) for x in train_prompts] for y in train_jobs_)))\n",
    "    test_examples = list(itertools.chain(*([x.replace('CEO', y) for x in test_prompts] for y in train_jobs_)))\n",
    "\n",
    "    percent = estimate_weight(model, train_examples, tokenizer, tok_train_jobs_, him_word, her_word)\n",
    "    percents.append(percent.item())\n",
    "    verbose=False\n",
    "    zeros.append(bias_fn(0, test_examples, model, tok_train_jobs_, him_word, her_word, tokenizer, verbose, regularize=0))\n",
    "    ones.append(bias_fn(1, test_examples, model, tok_train_jobs_, him_word, her_word, tokenizer, verbose, regularize=0))\n",
    "    minimized.append(bias_fn(percent, test_examples, model, tok_train_jobs_, him_word, her_word, tokenizer, verbose, regularize=0))\n",
    "\n",
    "avg_minimized = sum(percents)/len(percents)\n",
    "avg_minimized_biases = []\n",
    "for job in tok_train_jobs:\n",
    "    tok_trais_jobs_ = job\n",
    "    train_jobs_ = (tokenizer.decode(job).strip(),)\n",
    "    train_examples = list(itertools.chain(*([x.replace('CEO', y) for x in train_prompts] for y in train_jobs_)))\n",
    "    test_examples = list(itertools.chain(*([x.replace('CEO', y) for x in test_prompts] for y in train_jobs_)))\n",
    "    avg_minimized_biases.append(bias_fn(avg_minimized, test_examples, model, tok_train_jobs_, him_word, her_word, tokenizer, verbose, regularize=0))\n",
    "\n",
    "print('Ones', sum(ones)/len(ones), ones)\n",
    "print('Zeros', sum(zeros)/len(zeros), zeros)\n",
    "print('Minimized', sum(minimized)/len(minimized), minimized)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "de8054f9bcbabe1926e70ac2f2e529da79d26bb40aaf131602c25f4fe541c62a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
