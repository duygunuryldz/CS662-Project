{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation import SampleDecoderOnlyOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALING = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def senses_of_word(word, model):\n",
    "  tokens = (torch.ones(512)*word).reshape(1,512).long().cuda()\n",
    "  contents = model.backpack.sense_network(model.backpack.gpt2_model.wte(tokens)) #(bs, nv, s, d)\n",
    "  contents = contents[0,:,0,:] #(nv, d)\n",
    "  return contents\n",
    "\n",
    "\n",
    "def mogrify_word(model, word, out_word, in_word, tokenizer):\n",
    "\n",
    "  word = tokenizer(word)['input_ids'][0]\n",
    "  in_word = tokenizer(in_word)['input_ids'][0]\n",
    "  out_word = tokenizer(out_word)['input_ids'][0]\n",
    "  \n",
    "  def project_out_and_in(\n",
    "      senses, # (nv, d)\n",
    "      out_direction, # (d,)\n",
    "      in_direction, # (d,)\n",
    "      ):\n",
    "    #embeddings = embeddings.detach().clone()\n",
    "    dots = senses @ out_direction / (out_direction  @ out_direction) #(nv)\n",
    "    normalization = (out_direction @ out_direction) / (in_direction @ in_direction) #(1)\n",
    "    out_diffs = dots.unsqueeze(1) * out_direction.unsqueeze(0)  #(nv, d)\n",
    "    in_diffs = dots.unsqueeze(1) * in_direction.unsqueeze(0) * normalization * SCALING #(nv, d)\n",
    "    fixed_senses = senses - out_diffs + in_diffs\n",
    "    #for word in words:\n",
    "    #  embeddings[word[0]] = fixed_embeddings[word].type(embeddings.type()).detach()\n",
    "    return fixed_senses\n",
    "\n",
    "\n",
    "  word_senses = senses_of_word(word, model)\n",
    "  out_embedding_vector = model.lm_head.weight[out_word]\n",
    "  in_embedding_vector = model.lm_head.weight[in_word]\n",
    "  fixed_senses = project_out_and_in(word_senses, out_embedding_vector, in_embedding_vector)\n",
    "  #visualize_word(None, tokenizer, model, contents=fixed_senses)\n",
    "  return {word: fixed_senses}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_content(input_ids, content, sense_dict):\n",
    "    for batch_index in range(content.shape[0]):\n",
    "      for seq_index in range(content.shape[2]):\n",
    "        word = input_ids[batch_index][seq_index].detach().cpu().item()\n",
    "        if word in sense_dict:\n",
    "          content[batch_index, :, seq_index, :] = sense_dict[word]\n",
    "    return content\n",
    "\n",
    "\n",
    "def sample(input_ids, model, max_length, sense_dict, replace=True, sample=True):\n",
    "    \"\"\"Sampling. This is a very simple implementation.\n",
    "    We assume that all sequences in the same batch have the same length.\n",
    "    Arguments:\n",
    "        input_ids: (batch, seq_len)\n",
    "        max_length: int\n",
    "    Returns: GreedySearchDecoderOnlyOutput, with the following fields:\n",
    "        sequences: (batch, max_length)\n",
    "        scores: tuples of (batch, vocab_size)\n",
    "    \"\"\"\n",
    "    batch_size, seqlen_og = input_ids.shape\n",
    "    scores = []\n",
    "    with torch.inference_mode():\n",
    "        # unchanged\n",
    "        contextl_hidden_states = model.backpack.gpt2_model(input_ids, None)[\"last_hidden_state\"]\n",
    "        contextualization = model.backpack.sense_weight_net(contextl_hidden_states) # (bs, nv, s, s)\n",
    "\n",
    "        # Compute content and weight\n",
    "        content = model.backpack.sense_network(model.backpack.gpt2_model.wte(input_ids)) # (bs, nv, s, d)\n",
    "        if replace:\n",
    "          content = replace_content(input_ids, content, sense_dict)\n",
    "\n",
    "        # Compute resulting outputs\n",
    "        hidden_states = torch.sum(contextualization @ content, dim=1) # (bs, s, d)\n",
    "        logits = model.lm_head(hidden_states)[:, -1]\n",
    "\n",
    "        scores.append(logits)\n",
    "        if sample:\n",
    "            next_token = torch.distributions.Categorical(logits=torch.log_softmax(logits,dim=-1)).sample()\n",
    "        else:\n",
    "            next_token = torch.argmax(torch.log_softmax(logits,dim=-1), dim=1)\n",
    "        sequences = [next_token]\n",
    "        seqlen = seqlen_og+1\n",
    "        while seqlen < max_length:\n",
    "            input_ids = torch.cat((input_ids, next_token.unsqueeze(1)), dim=1)\n",
    "            logits = model(input_ids).logits[:, -1]\n",
    "            if sample:\n",
    "                next_token = torch.distributions.Categorical(logits=torch.log_softmax(logits,dim=-1)).sample()\n",
    "            else:\n",
    "                next_token = torch.argmax(torch.log_softmax(logits,dim=-1), dim=1)\n",
    "            seqlen += 1\n",
    "    return SampleDecoderOnlyOutput(\n",
    "        sequences=input_ids,\n",
    "        scores=tuple(scores)\n",
    "    )\n",
    "\n",
    "#same function with sample(input_ids, model, max_length, sense_dict, replace=False)\n",
    "def generate(input_ids, model, max_length, sample=True):\n",
    "    batch_size, seqlen_og = input_ids.shape\n",
    "    with torch.inference_mode():\n",
    "        logits = model(input_ids).logits[:, -1]\n",
    "        if sample:\n",
    "            next_token = torch.distributions.Categorical(logits=torch.log_softmax(logits,dim=-1)).sample()\n",
    "        else:\n",
    "            next_token = torch.argmax(torch.log_softmax(logits,dim=-1), dim=1)\n",
    "        seqlen = seqlen_og+1\n",
    "        while seqlen < max_length:\n",
    "            input_ids = torch.cat((input_ids, next_token.unsqueeze(1)), dim=1)\n",
    "            logits = model(input_ids).logits[:, -1]\n",
    "            if sample:\n",
    "                next_token = torch.distributions.Categorical(logits=torch.log_softmax(logits,dim=-1)).sample()\n",
    "            else:\n",
    "                next_token = torch.argmax(torch.log_softmax(logits,dim=-1), dim=1)\n",
    "            seqlen += 1\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_reduction_ce = nn.CrossEntropyLoss(reduction='none')\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "\n",
    "model_id = \"stanfordnlp/backpack-gpt2\"\n",
    "config = AutoConfig.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, config=config, trust_remote_code=True).cuda()\n",
    "model.eval()\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulate knowledge \n",
    "inp = \"The MacBook is best known for \"\n",
    "sense_dict = mogrify_word(\n",
    "    model,\n",
    "    ' MacBook',\n",
    "    ' Apple',\n",
    "    ' HP',\n",
    "    tokenizer)\n",
    "\n",
    "inp = torch.tensor(tokenizer(inp)['input_ids']).unsqueeze(0).to('cuda')\n",
    "outputs = sample(inp, model, 100, sense_dict).sequences\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Manipulate knowledge\n",
    "inp = \"The color of the sky is \"\n",
    "sense_dict = mogrify_word(\n",
    "    model,\n",
    "    ' sky',\n",
    "    ' blue',\n",
    "    ' green',\n",
    "    tokenizer)\n",
    "\n",
    "inp = torch.tensor(tokenizer(inp)['input_ids']).unsqueeze(0).to('cuda')\n",
    "outputs = sample(inp, model, 100, sense_dict).sequences\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't manipulate knowledge\n",
    "inp = \"The MacBook is best known for \"\n",
    "sense_dict = mogrify_word(\n",
    "    model,\n",
    "    ' MacBook',\n",
    "    ' Apple',\n",
    "    ' HP',\n",
    "    tokenizer)\n",
    "\n",
    "inp = torch.tensor(tokenizer(inp)['input_ids']).unsqueeze(0).to('cuda')\n",
    "outputs = sample(inp, model, 100, sense_dict, replace=False).sequences\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Don't manipulate knowledge\n",
    "inp = \"The MacBook is best known for \"\n",
    "inp = torch.tensor(tokenizer(inp)['input_ids']).unsqueeze(0).to('cuda')\n",
    "outputs = generate(inp, model, 100, sample=True)\n",
    "\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=False)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e16a30b5dbfefa3dcf05db8176214200dd9093f1cb4ebd65888ec5c0fb748ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
